{"pageContext":{"version":"0.5","versions":{"releases":["1.0","0.9","0.8","0.7","0.6","0.5","0.4"],"branches":["master"]},"content":{"id":"kyma","displayName":"Kyma","description":"Overall documentation for Kyma","type":"root","docs":[{"order":"001-overview-in-a-nutshell","title":"In a nutshell","source":"\nKyma allows you to connect applications and third-party services in a cloud-native environment. Use it to create extensions for the existing systems, regardless of the language they are written in. Customize extensions with minimum effort and time devoted to learning their configuration details.\n\nWith Kyma in hand, you can focus purely on coding since it ensures the following out-of-the-box functionalities:\n- Service-to-service communication and proxying (Istio Service Mesh)\n- In-built monitoring, tracing, and logging (Grafana, Prometheus, Jaeger, Logspout, OK Log)\n- Secure authentication and authorization (Dex, Service Identity, TLS, Role Based Access Control)\n- The catalog of services to choose from (Service Catalog, Service Brokers)\n- The development platform to run lightweight functions in a cost-efficient and scalable way (Serverless, Kubeless)\n- The endpoint to register Events and APIs of external applications (Application Connector)\n- The messaging channel to receive Events, enrich them, and trigger business flows using lambdas or services (Event Bus, NATS)\n- CLI supported by the intuitive UI (Console)\n","type":"Overview"},{"order":"002-overview-main-features","title":"Main features","source":"\nMajor open-source and cloud-native projects, such as Istio, NATS, Kubeless, and Prometheus, constitute the cornerstone of Kyma. Its uniqueness, however, lies in the \"glue\" that holds these components together. Kyma collects those cutting-edge solutions in one place and combines them with the in-house developed features that allow you to connect and extend your enterprise applications easily and intuitively.\n\nKyma allows you to extend and customize the functionality of your products in a quick and modern way, using serverless computing or microservice architecture. The extensions and customizations you create are decoupled from the core applications, which means that:\n- Deployments are quick.\n- Scaling is independent from the core applications.\n- The changes you make can be easily reverted without causing downtime of the production system.\n\nLast but not least, Kyma is highly cost-efficient. All Kyma native components and the connected open-source tools are written in Go. It ensures low memory consumption and reduced maintenance costs compared to applications written in other programming languages such as Java.\n","type":"Overview"},{"order":"003-overview-technology-stack","title":"Technology stack","source":"\nThe entire solution is containerized and runs on a [Kubernetes](https://kubernetes.io/) cluster. Customers can access it easily using a single sign on solution based on the [Dex](https://github.com/coreos/dex) identity provider integrated with any [OpenID Connect](https://openid.net/connect/)-compliant identity provider or a SAML2-based enterprise authentication server.\n\nThe communication between services is handled by the [Istio](https://istio.io/) Service Mesh component which enables security, traffic management, routing, resilience (retry, circuit breaker, timeouts), monitoring, and tracing without the need to change the application code.\nBuild your applications using services provisioned by one of the many Service Brokers compatible with the [Open Service Broker API](https://www.openservicebrokerapi.org/), and monitor the speed and efficiency of your solutions using [Prometheus](https://prometheus.io/), which gives you the most accurate and up-to-date monitoring data.\n","type":"Overview"},{"order":"004-overview-key-components","title":"Key components","source":"\nKyma is built of numerous components but these three drive it forward:\n\n  - **Application Connector** connects external on-premise or cloud-native applications with Kyma and registers their APIs and Events that internal applications or functions can later consume.\n  - **Serverless** is based on [Kubeless](https://kubeless.io/) and allows you to write short code snippets, known as functions or lambdas, that consume the Events exposed by the Application Connector and use external services provided by the Service Catalog to trigger certain business logic.\n  - **Service Catalog** offers third-party services in the form of ServiceClasses exposed by the Service Brokers that a given function can consume to perform certain actions.\n\nThis basic use case shows how the three components work together in Kyma:\n\n![key-components](./assets/ac-s-sc.svg)\n","type":"Overview"},{"order":"005-overview-kyma-and-knative","title":"Kyma and Knative - brothers in arms","source":"\nIntegration with Knative is a step towards Kyma modularization and the \"slimming\" approach which aims to extract some out-of-the-box components and provide you with a more flexible choice of tools to use in Kyma.\n\nBoth Kyma and Knative are Kubernetes and Istio-based systems that offer development and eventing platforms. The main difference, however, is their focus. While Knative concentrates more on providing the building blocks for running serverless workloads, Kyma focuses on integrating those blocks with external services and applications.\n\nThe diagram shows dependencies between the components:\n\n![kyma-knative](./assets/kyma-knative.svg)\n\nThe nearest plan for Kyma and Knative cooperation is to replace Serverless in Kyma with the Knative technology. Other planned changes concerning Kyma and Knative cooperation involve providing configuration options to allow Istio deployed with Knative to work on Kyma, and extracting Kyma eventing to fully integrate it with Knative eventing. The eventing integration will provide more flexibility on deciding which messaging implementation to use (NATS, Kafka, or any other).\n","type":"Overview"},{"order":"006-overview-how-to-start","title":"How to start","source":"\nWhen you already know what Kyma is and what components it consists of, you can start using it. Minikube allows you to run Kyma locally, develop, and test your solutions on a small scale before you push them to a cluster. With the Getting Started guides in hand, you can start developing in a matter of minutes.\n\nRead, learn, and try on your own to:\n- [Install Kyma locally](#installation-install-kyma-locally-from-the-release)\n- [Install Kyma on a cluster](#installation-install-kyma-on-a-gke-cluster)\n- [Deploy a sample service locally](#getting-started-sample-service-deployment-on-local)\n- [Deploy a service on a cluster](#getting-started-sample-service-deployment-on-a-cluster)\n- [Develop a service locally without using Docker](#getting-started-develop-a-service-locally-without-using-docker)\n- [Publish a service Docker image and deploy it to Kyma](#getting-started-publish-a-service-docker-image-and-deploy-it-to-kyma)\n- [Set up an external system on the local Kyma installation](/docs/0.5/components/application-connector#getting-started-create-a-new-remote-environment)\n- [Activate an external system](/docs/0.5/components/application-connector#getting-started-bind-a-remote-environment-to-an-environment)\n- [Expose custom metrics in Kyma](/docs/0.5/components/monitoring#getting-started-expose-custom-metrics-in-kyma)\n","type":"Overview"},{"order":"010-details-components","title":"Components","source":"\nKyma is built on the foundation of the best and most advanced open-source projects which make up the components readily available for customers to use.\nThis section describes the Kyma components.\n\n## Service Catalog\n\nThe Service Catalog lists all of the services available to Kyma users through the registered Service Brokers. Using the Service Catalog, you can provision new services in the\nKyma [Kubernetes](https://kubernetes.io/) cluster and create bindings between the provisioned service and an application.\n\n## Service Mesh\n\nThe Service Mesh is an infrastructure layer that handles service-to-service communication, proxying, service discovery, traceability, and security independent of the code of the services. Kyma uses the [Istio](https://istio.io/) Service Mesh that is customized for the specific needs of the implementation.\n\n## Security\n\nKyma security enforces RBAC (Role Based Access Control) in the cluster. [Dex](https://github.com/coreos/dex) handles the identity management and identity provider integration. It allows you to integrate any [OpenID Connect](https://openid.net/connect/) or SAML2-compliant identity provider with Kyma using [connectors](https://github.com/coreos/dex#connectors). Additionally, Dex provides a static user store which gives you more flexibility when managing access to your cluster.   \n\n## Service Brokers\n\nService Brokers are [Open Service Broker API](https://www.openservicebrokerapi.org/)-compatible servers that manage the lifecycle of one or more services. Each Service Broker registered in Kyma presents the services it offers to the Service Catalog. You can provision these services on a cluster level through the Service Catalog. Out of the box, Kyma comes with three Service Brokers.\nYou can register more [Open Service Broker API](https://www.openservicebrokerapi.org/)-compatible Service Brokers in Kyma and provision the services they offer using the Service Catalog.\n\n## Application Connector\n\nThe Application Connector is a proprietary Kyma solution. This endpoint is the Kyma side of the connection between Kyma and the external solutions. The Application Connector allows you to register the APIs and the Event Catalog, which lists all of the available events, of the connected solution. Additionally, the Application Connector proxies the calls from Kyma to external APIs in a secure way.\n\n## Event Bus\n\nKyma Event Bus receives Events from external solutions and triggers the business logic created with lambda functions and services in Kyma. The Event Bus is based on the [NATS Streaming](https://nats.io/) open source messaging system for cloud-native applications.\n\n## Serverless\n\nThe Kyma Serverless component allows you to reduce the implementation and operation effort of an application to the absolute minimum. Kyma Serverless provides a platform to run lightweight functions in a cost-efficient and scalable way using JavaScript and Node.js. Kyma Serverless is built on the [Kubeless](http://kubeless.io/) framework, which allows you to deploy lambda functions,\nand uses the [NATS](https://nats.io/) messaging system that monitors business events and triggers functions accordingly.  \n\n## Monitoring\n\nKyma comes bundled with tools that give you the most accurate and up-to-date monitoring data. [Prometheus](https://prometheus.io/) open source monitoring and alerting toolkit provides this data, which is consumed by different add-ons, including [Grafana](https://grafana.com/) for analytics and monitoring, and [Alertmanager](https://prometheus.io/docs/alerting/alertmanager/) for handling alerts.\n\n## Tracing\n\nThe tracing in Kyma uses the [Jaeger](https://github.com/jaegertracing) distributed tracing system. Use it to analyze performance by scrutinizing the path of the requests sent to and from your service. This information helps you optimize the latency and performance of your solution.\n\n## Logging\n\nLogging in Kyma uses [Logspout](https://github.com/gliderlabs/logspout) and [OK Log](https://github.com/oklog/oklog). Use a plaintext or a regular expression to fetch logs from Pods using the OK Log UI.\n","type":"Details"},{"order":"011-details-environments","title":"Environments","source":"\nAn Environment is a custom Kyma security and organizational unit based on the concept of Kubernetes [Namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/). Kyma Environments allow you to divide the cluster\ninto smaller units to use for different purposes, such as development and testing.\n\nKyma Environment is a user-created Namespace marked with the `env: \"true\"` label. The Kyma UI only displays the Namespaces marked with the `env: \"true\"` label.\n\n\n## Default Kyma Namespaces\n\nKyma comes configured with default Namespaces dedicated for system-related purposes. The user cannot modify or remove any of these Namespaces.\n\n- `kyma-system` - This Namespace contains all of the Kyma Core components.\n- `kyma-integration` - This Namespace contains all of the Application Connector components responsible for the integration of Kyma and external solutions.\n- `kyma-installer` - This Namespace contains all of the Kyma installer components, objects, and Secrets.\n- `istio-system` - This Namespace contains all of the Istio-related components.\n\n## Environments in Kyma\n\nKyma comes with three Environments ready for you to use. These environments are:\n\n- `production`\n- `qa`\n- `stage`\n\n## Create a new Environment\n\nTo create a new Environment, create a Namespace and mark it with the `env: \"true\"` label. Use this command to do that in a single step:\n\n```\n$ cat <<EOF | kubectl create -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-environment\n  labels:\n    env: \"true\"\nEOF\n```\n\nInitially, the system deploys two template roles: `kyma-reader-role` and `kyma-admin-role`. The controller finds the template roles by filtering available roles in the namespace `kyma-system` by the label `env: \"true\"`. The controller copies these roles into the Environment.\n","type":"Details"},{"order":"012-details-testing","title":"Testing Kyma","source":"\nFor testing, the Kyma components use the Helm test concept. Place your test under the `templates` directory as a Pod definition that specifies a container with a given command to run.\n\n## Add a new test\n\nThe system bases tests on the Helm broker concept with one modification: adding a Pod label. Before you create a test, see the official [Chart Tests](https://github.com/kubernetes/helm/blob/release-2.10/docs/chart_tests.md) documentation. Then, add the `\"helm-chart-test\": \"true\"` label to your Pod template.\n\nSee the following example of a test prepared for Dex:\n\n```\n# Chart tree\ndex\n├── Chart.yaml\n├── README.md\n├── templates\n│   ├── tests\n│   │   └── test-dex-connection.yaml\n│   ├── dex-deployment.yaml\n│   ├── dex-ingress.yaml\n│   ├── dex-rbac-role.yaml\n│   ├── dex-service.yaml\n│   ├── pre-install-dex-account.yaml\n│   ├── pre-install-dex-config-map.yaml\n│   └── pre-install-dex-secrets.yaml\n└── values.yaml\n```\n\nThe test adds a new **test-dex-connection.yaml** under the `templates/tests` directory.\nThis simple test calls the `Dex` endpoint with cURL, defined as follows:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: \"test-{{ template \"fullname\" . }}-connection-dex\"\n  annotations:\n    \"helm.sh/hook\": test-success\n  labels:\n      \"helm-chart-test\": \"true\" # ! Our customization\nspec:\n  hostNetwork: true\n  containers:\n  - name: \"test-{{ template \"fullname\" . }}-connection-dex\"\n    image: tutum/curl:alpine\n    command: [\"/usr/bin/curl\"]\n    args: [\n      \"--fail\",\n      \"http://dex-service.{{ .Release.Namespace }}.svc.cluster.local:5556/.well-known/openid-configuration\"\n    ]\n  restartPolicy: Never\n```\n\n## Test execution\n\nAll tests created for charts under `/resources/core/` run automatically after starting Kyma.\nIf any of the tests fail, the system prints the Pod logs in the terminal, then deletes all the Pods.\n\n>**NOTE:** If you run Kyma locally, by default, the system does not take into account the test's exit code. As a result, the system does not terminate Kyma Docker container, and you can still access it.\nTo force a termination in case of failing tests, use `--exit-on-test-fail` flag when executing `run.sh` script.\n\nCI propagates the exit status of tests. If any test fails, the whole CI job fails as well.\n\nFollow the same guidelines to add a test which is not a part of any `core` component. However, for test execution, see the **Run a test manually** section in this document.\n\n### Run a test manually\n\nTo run a test manually, use the `testing.sh` script located in the `/installation/scripts/` directory which runs all tests defined for `core` releases.\nIf any of the tests fail, the system prints the Pod logs in the terminal, then deletes all the Pods.\n\nAnother option is to run a Helm test directly on your release.\n\n```bash\n$ helm test {your_release_name}\n```\n\nYou can also run your test on custom releases. If you do this, remember to always delete the Pods after a test ends.\n","type":"Details"},{"order":"013-details-charts","title":"Charts","source":"\nKyma uses Helm charts to deliver single components and extensions, as well as the core components. This document contains information about the chart-related technical concepts, dependency management to use with Helm charts, and chart examples.\n\n## Manage dependencies with Init Containers\n\nThe **ADR 003: Init Containers for dependency management** document declares the use of Init Containers as the primary dependency mechanism.\n\n[Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) present a set of distinctive behaviors:\n\n* They always run to completion.\n* They start sequentially, only after the preceding Init Container completes successfully.\n  If any of the Init Containers fails, the Pod restarts. This is always true, unless the `restartPolicy` equals `never`.\n\n[Readiness Probes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes) ensure that the essential containers are ready to handle requests before you expose them. At a minimum, probes are defined for every container accessible from outside of the Pod. It is recommended to pair the Init Containers with readiness probes to provide a basic dependency management solution.\n\n## Examples\nHere are some examples:\n\n1. Generic\n\n\n```yaml\napiVersion: apps/v1beta2\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n```\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\nspec:\n  initContainers:\n  - name: init-myservice\n    image: busybox\n    command: ['sh', '-c', 'until nslookup nginx; do echo waiting for nginx; sleep 2; done;']\n  containers:\n  - name: myapp-container\n    image: busybox\n    command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n```\n\n2. Kyma\n\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: helm-broker\n  labels:\n    app: helm-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: helm-broker\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: helm-broker\n    spec:\n\n      initContainers:\n      - name: init-helm-broker\n        image: eu.gcr.io/kyma-project/alpine-net:0.2.74\n        command: ['sh', '-c', 'until nc -zv core-catalog-controller-manager.kyma-system.svc.cluster.local 8080; do echo waiting for etcd service; sleep 2; done;']\n\n      containers:\n      - name: helm-broker\n        ports:\n        - containerPort: 6699\n        readinessProbe:\n          tcpSocket:\n            port: 6699\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 2\n```\n\n## Support for the Helm wait flag\n\nHigh level Kyma components, such as **core**, come as Helm charts. These charts are installed as part of a single Helm release. To provide ordering for these core components, the Helm client runs with the `--wait` flag. As a result, Tiller waits for the readiness of all of the components, and then evaluates the readiness.\n\nFor `Deployments`, set the strategy to `RollingUpdate` and set the `MaxUnavailable` value to a number lower than the number of replicas. This setting is necessary, as readiness in Helm v2.10.0 is fulfilled if the number of replicas in ready state is not lower than the expected number of replicas:\n\n```\nReadyReplicas >= TotalReplicas - MaxUnavailable\n```\n\n## Chart installation details\n\nThe Tiller server performs the chart installation process. This is the order of operations that happen during the chart installation:\n\n* resolve values\n* recursively gather all templates with the corresponding values\n* sort all templates\n* render all templates\n* separate hooks and manifests from files into sorted lists\n* aggregate all valid manifests from all sub-charts into a single manifest file\n* execute PreInstall hooks\n* create a release using the ReleaseModule API and, if requested, wait for the actual readiness of the resources\n* execute PostInstall hooks\n\n## Notes\n\nAll notes are based on Helm v2.10.0 implementation and are subject to change in feature releases.\n\n* Regardless of how complex a chart is, and regardless of the number of sub-charts it references or consists of, it's always evaluated as one. This means that each Helm release is compiled into a single Kubernetes manifest file when applied on API server.\n\n* Hooks are parsed in the same order as manifest files and returned as a single, global list for the entire chart. For each hook the weight is calculated as a part of this sort.\n\n* Manifests are sorted by `Kind`. You can find the list and the order of the resources on the Kubernetes [Tiller](https://github.com/kubernetes/helm/blob/v2.10.0/pkg/tiller/kind_sorter.go#L29) website.\n\n## Glossary\n\n* **resource** is any document in a chart recognized by Helm or Tiller. This includes manifests, hooks, and notes.\n* **template** is a valid Go template. Many of the resources are also Go templates.\n","type":"Details"},{"order":"014-details-deploy-private-registry","title":"Deploy with a private Docker registry","source":"\nDocker is a free tool to deploy applications and servers. To run an application on Kyma, provide the application binary file as a Docker image located in a Docker registry. Use the `DockerHub` public registry to upload your Docker images for free access to the public. Use a private Docker registry to ensure privacy, increased security, and better availability.\n\nThis document shows how to deploy a Docker image from your private Docker registry to the Kyma cluster.\n\n## Details\n\nThe deployment to Kyma from a private registry differs from the deployment from a public registry. You must provide Secrets accessible in Kyma, and referenced in the `.yaml` deployment file. This section describes how to deploy an image from a private Docker registry to Kyma. Follow the deployment steps:\n\n1. Create a Secret resource.\n2. Write your deployment file.\n3. Submit the file to the Kyma cluster.\n\n### Create a Secret for your private registry\n\nA Secret resource passes your Docker registry credentials to the Kyma cluster in an encrypted form. For more information on Secrets, refer to the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/secret/).\n\nTo create a Secret resource for your Docker registry, run the following command:\n\n```bash\nkubectl create secret docker-registry {secret-name} --docker-server={registry FQN} --docker-username={user-name} --docker-password={password} --docker-email={registry-email} --namespace={namespace}  \n```\n\nRefer to the following example:\n```bash\nkubectl create secret docker-registry docker-registry-secret --docker-server=myregistry:5000 --docker-username=root --docker-password=password --docker-email=example@github.com --namespace=production\n```\n\nThe Secret is associated with a specific Namespace. In the example, the Namespace is `production`. However, you can modify the Secret to point to any desired Namespace.\n\n### Write your deployment file\n\n1. Create the deployment file with the `.yml` extension and name it `deployment.yml`.\n\n2. Describe your deployment in the `.yml` file. Refer to the following example:\n\n```yaml\napiVersion: apps/v1beta2\nkind: Deployment\nmetadata:\n  namespace: production # {production/stage/qa}\n  name: my-deployment # Specify the deployment name.\n  annotations:\n    sidecar.istio.io/inject: true\nspec:\n  replicas: 3 # Specify your replica - how many instances you want from that deployment.\n  selector:\n    matchLabels:\n      app: app-name # Specify the app label. It is optional but it is a good practice.\n  template:\n    metadata:\n      labels:\n        app: app-name # Specify app label. It is optional but it is a good practice.\n        version: v1 # Specify your version.\n    spec:\n      containers:\n      - name: container-name # Specify a meaningful container name.\n        image: myregistry:5000/user-name/image-name:latest # Specify your image {registry FQN/your-username/your-space/image-name:image-version}.\n        ports:\n          - containerPort: 80 # Specify the port to your image.\n      imagePullSecrets:\n        - name: docker-registry-secret # Specify the same Secret name you generated in the previous step for this Namespace.\n        - name: example-secret-name # Specify your Namespace Secret, named `example-secret-name`.\n\n```\n3. Submit you deployment file using this command:\n\n```bash\nkubectl apply -f deployment.yml\n```\nYour deployment is now running on the Kyma cluster.\n","type":"Details"},{"order":"030-inst-local-installation-from-release","title":"Install Kyma locally from the release","source":"\nThis Installation guide shows developers how to quickly deploy Kyma locally on a Mac or Linux from the latest release. Kyma installs locally using a proprietary installer based on a [Kubernetes operator](https://coreos.com/operators/). The document provides prerequisites, instructions on how to install Kyma locally and verify the deployment, as well as the troubleshooting tips.\n\n## Prerequisites\n\nTo run Kyma locally, clone this Git repository to your machine and check out the latest release.\n\nAdditionally, download these tools:\n\n- [Minikube](https://github.com/kubernetes/minikube) 0.28.2\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.10.0\n- [Helm](https://github.com/kubernetes/helm) 2.10.0\n- [jq](https://stedolan.github.io/jq/)\n\nVirtualization:\n\n- [Hyperkit driver](https://github.com/kubernetes/minikube/blob/master/docs/drivers.md#hyperkit-driver) - Mac only\n- [VirtualBox](https://www.virtualbox.org/) - Linux only\n\n> **NOTE:** To work with Kyma, use only the provided scripts and commands. Kyma does not work on a basic Minikube cluster that you can start using the `minikube start` command or stop with the `minikube stop` command. If you don't need Kyma on Minikube anymore, remove the cluster with the `minikube delete` command.\n\n## Set up certificates\n\nKyma comes with a local wildcard self-signed `server.crt` certificate that you can find under the `/installation/certs/workspace/raw/` directory of the `kyma` repository. Trust it on the OS level for convenience.\n\nFollow these steps to \"always trust\" the Kyma certificate on Mac:\n\n1. Change the working directory to `installation`:\n  ```\n  cd installation\n  ```\n2. Run this command:\n  ```\n  sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain certs/workspace/raw/server.crt\n  ```\n\n>**NOTE:** \"Always trusting\" the certificate does not work with Mozilla Firefox.\n\nTo access the Application Connector and connect an external solution to the local deployment of Kyma, you must add the certificate to the trusted certificate storage of your programming environment. Read the **Access Application Connector on local Kyma** in the **Application Connector** topic to learn more.\n\n## Install Kyma on Minikube\n\nYou can install Kyma either with all core subcomponents or only with the selected ones. This section describes how to install Kyma with all core subcomponents. To learn how to install only the specific ones, see the **Install subcomponents** document for details.\n\n> **NOTE:** Running the installation script deletes any previously existing cluster from your Minikube.\n\nTo install Kyma, follow these steps:\n\n1. Change the working directory to `installation`:\n  ```\n  cd installation\n  ```\n\n2. Use the following command to run Kubernetes locally using Minikube:\n```\n$ ./scripts/minikube.sh --domain \"kyma.local\" --vm-driver \"hyperkit\"\n```\n\n3. Kyma installation requires increased permissions granted by the **cluster-admin** role. To bind the role to the default **ServiceAccount**, run the following command:\n```\n$ kubectl apply -f ./resources/default-sa-rbac-role.yaml\n```\n\n4. Wait until the `kube-dns` Pod is ready. Run this script to setup Tiller:\n```\n$ ./scripts/install-tiller.sh\n```\n\n5. Configure the Kyma installation using the local configuration file from the 0.5.0 release:\n```\n$ kubectl apply -f https://github.com/kyma-project/kyma/releases/download/0.5.0/kyma-config-local.yaml\n```\n\n6. To trigger the installation process, label the `kyma-installation` custom resource:\n```\n$ kubectl label installation/kyma-installation action=install\n```\n\n7. By default, the Kyma installation is a background process, which allows you to perform other tasks in the terminal window. Nevertheless, you can track the progress of the installation by running this script:\n```\n$ ./scripts/is-installed.sh\n```\n\nRead the **Reinstall Kyma** document to learn how to reinstall Kyma without deleting the cluster from Minikube.\nTo learn how to test Kyma, see the **Testing Kyma** document.\n\n## Verify the deployment\n\nFollow the guidelines in the subsections to confirm that your Kubernetes API Server is up and running as expected.\n\n### Access Kyma with CLI\n\nVerify the cluster deployment with the kubectl command line interface (CLI).\n\nRun this command to fetch all Pods in all Namespaces:\n\n  ``` bash\n  kubectl get pods --all-namespaces\n  ```\nThe command retrieves all Pods from all Namespaces, the status of the Pods, and their instance numbers. Check if the **STATUS** column shows `Running` for all Pods. If any of the Pods that you require do not start successfully, perform the installation again.\n\n### Access the Kyma console\n\nAccess your local Kyma instance through [this](https://console.kyma.local/) link.\n\n* Click **Login with Email** and sign in with the `admin@kyma.cx` email address. Use the password contained in the  `admin-user` Secret located in the `kyma-system` Namespace. To get the password, run:\n\n``` bash\nkubectl get secret admin-user -n kyma-system -o jsonpath=\"{.data.password}\" | base64 -D\n```\n\n* Click the **Environments** section and select an Environment from the drop-down menu to explore Kyma further.\n\n### Access the Kubernetes Dashboard\n\nAdditionally, confirm that you can access your Kubernetes Dashboard. Run the following command to check the IP address on which Minikube is running:\n\n```bash\nminikube ip\n```\n\nThe address of your Kubernetes Dashboard looks similar to this:\n```\nhttp://{ip-address}:30000\n```\n\nSee the example of the website address:\n\n```\nhttp://192.168.64.44:30000\n```\n\n## Troubleshooting\n\nIf the installer does not respond as expected, check the installation status using the `is-installed.sh` script with the `--verbose` flag added. Run:\n```\nscripts/is-installed.sh --verbose\n```\n","type":"Installation"},{"order":"031-inst-local-installation-from-sources","title":"Install Kyma locally from sources","source":"\nThis Installation guide shows developers how to quickly deploy Kyma on a Mac or Linux from local sources. Follow it if you want to use Kyma for development purposes.\n\nKyma installs locally using a proprietary installer based on a [Kubernetes operator](https://coreos.com/operators/). The document describes only the installation part. For prerequisites, certificates setup, deployment validation, and troubleshooting steps, see the **Install Kyma locally from the release** document.\n\n## Install Kyma\n\nTo run Kyma locally, clone this Git repository to your machine.\n\nTo start the local installation, run the following command:\n\n```\n./installation/cmd/run.sh\n```\n\nThis script sets up default parameters, starts Minikube, builds the Kyma-Installer, generates local configuration, creates the Installation custom resource, and sets up the Installer.\n\n> **NOTE:** See the **Local installation scripts** document for a detailed explanation of the `run.sh` script and the subscripts it triggers.\n\nYou can execute the `installation/cmd/run.sh` script with the following parameters:\n\n- `--skip-minikube-start` which skips the execution of the `installation/scripts/minikube.sh` script.\n- `--vm-driver` which points to either `virtualbox` or `hiperkit`, depending on your operating system.\n\nRead the **Reinstall Kyma** document to learn how to reinstall Kyma without deleting the cluster from Minikube.\nTo learn how to test Kyma, see the **Testing Kyma** document.\n","type":"Installation"},{"order":"032-inst-gke-installation","title":"Install Kyma on a GKE cluster","source":"\nThis Installation guide shows developers how to quickly deploy Kyma on a [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/) (GKE) cluster. Kyma installs on a cluster using a proprietary installer based on a Kubernetes operator.\n\n## Prerequisites\n- A domain for your GKE cluster\n- [Google Cloud Platform](https://console.cloud.google.com/) (GCP) project\n- [Docker](https://www.docker.com/)\n- [Docker Hub](https://hub.docker.com/) account\n- [gcloud](https://cloud.google.com/sdk/gcloud/)\n\n## DNS setup\n\nDelegate the management of your domain to Google Cloud DNS. Follow these steps:\n\n1. Export the domain name, project name, and DNS zone name as environment variables. Run the commands listed below:\n\n    ```\n    export DOMAIN={YOUR_SUBDOMAIN}\n    export DNS_NAME={YOUR_DOMAIN}.\n    export PROJECT={YOUR_GOOGLE_PROJECT}\n    export DNS_ZONE={YOUR_DNS_ZONE}\n    ```\n\n2. Create a DNS-managed zone in your Google project. Run:\n\n    ```\n    gcloud dns --project=$PROJECT managed-zones create $DNS_ZONE --description= --dns-name=$DNS_NAME\n    ```\n\n    Alternatively, create it through the GCP UI. Navigate go to **Network Services** in the **Network** section, click **Cloud DNS** and select **Create Zone**.\n\n3. Delegate your domain to Google name servers.\n\n    - Get the list of the name servers from the zone details. This is a sample list:\n      ```\n      ns-cloud-b1.googledomains.com.\n      ns-cloud-b2.googledomains.com.\n      ns-cloud-b3.googledomains.com.\n      ns-cloud-b4.googledomains.com.\n      ```\n\n    - Set up your domain to use these name servers.\n\n4. Check if everything is set up correctly and your domain is managed by Google name servers. Run:\n    ```\n    host -t ns $DNS_NAME\n    ```\n    A successful response returns the list of the name servers you fetched from GCP.\n\n## Get the TLS certificate\n\n1. Create a folder for certificates. Run:\n    ```\n    mkdir letsencrypt\n    ```\n2. Create a new service account and assign it to the `dns.admin` role. Run these commands:\n    ```\n    gcloud iam service-accounts create dnsmanager --display-name \"dnsmanager\"\n    ```\n    ```\n    gcloud projects add-iam-policy-binding $PROJECT \\\n        --member serviceAccount:dnsmanager@$PROJECT.iam.gserviceaccount.com --role roles/dns.admin\n    ```\n\n3. Generate an access key for this account in the `letsencrypt` folder. Run:\n    ```\n    gcloud iam service-accounts keys create ./letsencrypt/key.json --iam-account dnsmanager@$PROJECT.iam.gserviceaccount.com\n    ```\n4. Run the Certbot Docker image with the `letsencrypt` folder mounted. Certbot uses the key to apply DNS challenge for the certificate request and stores the TLS certificates in that folder. Run:\n    ```\n    docker run -it --name certbot --rm \\\n        -v \"$(pwd)/letsencrypt:/etc/letsencrypt\" \\\n        certbot/dns-google \\\n        certonly \\\n        -m YOUR_EMAIL_HERE --agree-tos --no-eff-email \\\n        --dns-google \\\n        --dns-google-credentials /etc/letsencrypt/key.json \\\n        --server https://acme-v02.api.letsencrypt.org/directory \\\n        -d \"*.$DOMAIN\"\n    ```\n\n5. Export the certificate and key as environment variables. Run these commands:\n\n    ```\n    export TLS_CERT=$(cat ./letsencrypt/live/$DOMAIN/fullchain.pem | base64 | sed 's/ /\\\\ /g')\n    export TLS_KEY=$(cat ./letsencrypt/live/$DOMAIN/privkey.pem | base64 | sed 's/ /\\\\ /g')\n    ```\n\n## Prepare the GKE cluster\n\n1. Select a name for your cluster and set it as an environment variable. Run:\n    ```\n    export CLUSTER_NAME={CLUSTER_NAME_YOU_WANT}\n    ```\n\n2. Create a cluster in the `europe-west1` region. Run:\n    ```\n    gcloud container --project \"$PROJECT\" clusters \\\n    create \"$CLUSTER_NAME\" --zone \"europe-west1-b\" \\\n    --cluster-version \"1.10.7\" --machine-type \"n1-standard-2\" \\\n    --addons HorizontalPodAutoscaling,HttpLoadBalancing,KubernetesDashboard\n    ```\n\n3. Install Tiller on your GKE cluster. Run:\n\n    ```\n    kubectl apply -f installation/resources/tiller.yaml\n    ```\n\n## Prepare the installation configuration file\n\n### Using the latest GitHub release\n\n1. Download the `kyma-config-cluster` file bundled with the latest Kyma [release](https://github.com/kyma-project/kyma/releases/). Run:\n   ```\n   LATEST=$(curl https://github.com/kyma-project/kyma/releases/latest -I|grep Location:| rev | cut -d'/' -f1 | rev|tr -d '\\r')\n   wget https://github.com/kyma-project/kyma/releases/download/$LATEST/kyma-config-cluster.yaml\n   ```\n\n2. Update the file with the values from your environment variables. Run:\n    ```\n    cat kyma-config-cluster.yaml | sed -e \"s/__DOMAIN__/$DOMAIN/g\" |sed -e \"s/__TLS_CERT__/$TLS_CERT/g\" | sed -e \"s/__TLS_KEY__/$TLS_KEY/g\"|sed -e \"s/__.*__//g\"  >my-kyma.yaml\n    ```\n\n3. The output of this operation is the `my_kyma.yaml` file. Use it to deploy Kyma on your GKE cluster.\n\n\n### Using your own image\n\n1. Checkout [kyma-project](https://github.com/kyma-project/kyma) and enter the root folder.\n\n2. Build an image that is based on the current installer image and includes the current installation and resources charts. Run:\n\n    ```\n    docker build -t kyma-installer:latest -f kyma-installer/kyma.Dockerfile . --build-arg INSTALLER_VERSION=63484523\n    ```\n\n3. Push the image to your Docker Hub:\n    ```\n    docker tag kyma-installer:latest [YOUR_DOCKER_LOGIN]/kyma-installer:latest\n    ```\n    ```\n    docker push [YOUR_DOCKER_LOGIN]/kyma-installer:latest\n    ```\n\n4. Prepare the deployment file:\n\n    ```\n    cat installation/resources/installer.yaml <(echo -e \"\\n---\") installation/resources/installer-config-cluster.yaml.tpl  <(echo -e \"\\n---\") installation/resources/installer-cr-cluster.yaml.tpl | sed -e \"s/__DOMAIN__/$DOMAIN/g\" |sed -e \"s/__TLS_CERT__/$TLS_CERT/g\" | sed -e \"s/__TLS_KEY__/$TLS_KEY/g\" | sed -e \"s/__.*__//g\" > my-kyma.yaml\n    ```\n\n5. The output of this operation is the `my_kyma.yaml` file. Modify it to fetch the proper image with the changes you made ([YOUR_DOCKER_LOGIN]/kyma-installer:latest). Use the modified file to deploy Kyma on your GKE cluster.\n\n\n## Deploy Kyma\n\n1. Configure kubectl to use your new cluster. Run:  add yourself as the cluster admin, and deploy Kyma installer with your configuration.\n    ```\n    gcloud container clusters get-credentials $CLUSTER_NAME --zone europe-west1-b --project $PROJECT\n    ```\n2. Add your account as the cluster administrator:\n    ```\n    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)\n    ```\n3. Deploy Kyma using the `my-kyma` custom configuration file you created. Run:\n    ```\n    kubectl apply -f my-kyma.yaml\n    ```\n4. Check if the Pods of Tiller and the Kyma installer are running:\n    ```\n    kubectl get pods --all-namespaces\n    ```\n\n5. Start Kyma installation:\n    ```\n    kubectl label installation/kyma-installation action=install\n    ```\n\n6. To watch the installation progress, run:\n    ```\n    kubectl get pods --all-namespaces -w\n    ```\n\n\n## Configure DNS for the cluster load balancer\n\nRun these commands:\n\n```\nexport EXTERNAL_PUBLIC_IP=$(kubectl get service -n istio-system istio-ingressgateway -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n\nexport REMOTE_ENV_IP=$(kubectl get service -n kyma-system application-connector-nginx-ingress-controller -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n\ngcloud dns --project=$PROJECT record-sets transaction start --zone=$DNS_ZONE\n\ngcloud dns --project=$PROJECT record-sets transaction add $EXTERNAL_PUBLIC_IP --name=\\*.$DOMAIN. --ttl=60 --type=A --zone=$DNS_ZONE\n\ngcloud dns --project=$PROJECT record-sets transaction add $REMOTE_ENV_IP --name=\\gateway.$DOMAIN. --ttl=60 --type=A --zone=$DNS_ZONE\n\ngcloud dns --project=$PROJECT record-sets transaction execute --zone=$DNS_ZONE\n\n```\n\n## Prepare your Kyma deployment for production use\n\nTo use the cluster in a production environment, it is recommended you configure a new server-side certificate for the Application Connector and replace the placeholder certificate it installs with.\nIf you don't generate a new certificate, the system uses the placeholder certificate. As a result, the security of your implementation is compromised.\n\nFollow this steps to configure a new, more secure certificate suitable for production use.\n\n1. Generate a new certificate and key. Run:\n\n    ```\n    openssl req -new -newkey rsa:4096 -nodes -keyout ca.key -out ca.csr -subj \"/C=PL/ST=N/L=GLIWICE/O=SAP Hybris/OU=Kyma/CN=wormhole.kyma.cx\"\n\n    openssl x509 -req -sha256 -days 365 -in ca.csr -signkey ca.key -out ca.pem\n    ```\n\n2. Export the certificate and key to environment variables:\n\n    ```\n    export AC_CRT=$(cat ./ca.pem | base64 | base64)\n    export AC_KEY=$(cat ./ca.key | base64 | base64)\n\n    ```\n\n3. Prepare installation file with the following command:\n\n    ```\n    cat kyma-config-cluster.yaml | sed -e \"s/__DOMAIN__/$DOMAIN/g\" |sed -e \"s/__TLS_CERT__/$TLS_CERT/g\" | sed -e \"s/__TLS_KEY__/$TLS_KEY/g\" | sed -e \"s/__REMOTE_ENV_CA__/$AC_CRT/g\" | sed -e \"s/__REMOTE_ENV_CA_KEY__/$AC_KEY/g\" |sed -e \"s/__.*__//g\"  >my-kyma.yaml\n    ```\n","type":"Installation"},{"order":"033-inst-local-installation-scripts","title":"Local installation scripts","source":"\nThis document extends the **Install Kyma locally from sources** guide with a detailed breakdown of the alternative local installation method which is the `run.sh` script.\n\nThe following snippet is the main element of the `run.sh` script:\n\n```\nif [[ ! $SKIP_MINIKUBE_START ]]; then\n    bash $CURRENT_DIR/../scripts/minikube.sh --domain \"$DOMAIN\" --vm-driver \"$VM_DRIVER\"\nfi\n\nbash $CURRENT_DIR/../scripts/build-kyma-installer.sh --vm-driver \"$VM_DRIVER\"\n\nbash $CURRENT_DIR/../scripts/generate-local-config.sh\n\nif [ -z \"$CR_PATH\" ]; then\n\n    TMPDIR=`mktemp -d \"$CURRENT_DIR/../../temp-XXXXXXXXXX\"`\n    CR_PATH=\"$TMPDIR/installer-cr-local.yaml\"\n\n    bash $CURRENT_DIR/../scripts/create-cr.sh --output \"$CR_PATH\" --domain \"$DOMAIN\"\n    bash $CURRENT_DIR/../scripts/installer.sh --local --cr \"$CR_PATH\"\n\n    rm -rf $TMPDIR\nelse\n    bash $CURRENT_DIR/../scripts/installer.sh --cr \"$CR_PATH\"\nfi\n```\nSubsequent sections provide details of all involved subscripts, in the order in which the `run.sh` script triggers them.\n\n## The minikube.sh script\n\n> **NOTE:** To work with Kyma, use only the provided scripts and commands. Kyma does not work on a basic Minikube cluster that you can start using the `minikube start` command or stop with the `minikube stop` command. If you don't need Kyma on Minikube anymore, remove the cluster with the `minikube delete` command.\n\nThe purpose of the `installation/scripts/minikube.sh` script is to configure and start Minikube. The script also checks if your development environment is configured to handle the Kyma installation. This includes checking Minikube and kubectl versions. If Minikube is already initialized, the system prompts you to agree to remove the previous Minikube cluster. The script exits if you do not want to restart your cluster.\n\nMinikube is configured to disable the default Nginx Ingress Controller.\n\n>**NOTE:** For the complete list of parameters passed to the `minikube start` command, refer to the `installation/scripts/minikube.sh` script.\n\nOnce Minikube is up and running, the script adds local installation entries to `/etc/hosts`.\n\n## The build-kyma-installer.sh script\n\nThe Installer is an application based on a [Kubernetes operator](https://coreos.com/operators/). Its purpose is to install Helm charts defined in the Installation custom resource. The Kyma-Installer is a Docker image that bundles the Installer binary with Kyma charts.\n\nThe `installation/scripts/build-kyma-installer.sh` script extracts the Kyma-Installer image name from the `installer.yaml` deployment file and uses it to build a Docker image inside Minikube. This image contains local Kyma sources from the `resources` folder.\n\n>**NOTE:** For the Kyma-Installer Docker image details, refer to the `kyma-installer/kyma.Dockerfile` file.\n\n## The generate-local-config.sh script\n\nThe `generate-local-config.sh` script configures optional subcomponents. At the moment, only the Azure Broker is an optional subcomponent of the `core` deployment.\n\nThe Azure Broker subcomponent is part of the `core` deployment that provisions managed services in the Microsoft Azure cloud. To enable the Azure Broker, export the following environment variables:\n - AZURE_BROKER_SUBSCRIPTION_ID\n - AZURE_BROKER_TENANT_ID\n - AZURE_BROKER_CLIENT_ID\n - AZURE_BROKER_CLIENT_SECRET\n\n>**NOTE:** You need to export above environment variables before executing the `installation/cmd/run.sh` script. As the Azure credentials are converted to a Kubernetes Secret, make sure the exported values are base64-encoded.\n\n## The create-cr.sh script\n\nThe `installation/scripts/create-cr.sh` script prepares the Installation custom resource from the `installation/resources/installer-cr.yaml.tpl` template. The local installation scenario uses the default Installation custom resource. The Kyma-Installer already contains local Kyma resources bundled, thus `url` is ignored by the Installer component.\n\n>**NOTE:** For the Installation custom resource details, refer to the **Installation** document.\n\n## The installer.sh script\n\nThe `installation/scripts/installer.sh` script creates the default RBAC role, installs [Tiller](https://docs.helm.sh/), and deploys the Kyma-Installer component.\n\n>**NOTE:** For the Kyma-Installer deployment details, refer to the `installation/resources/installer.yaml` file.\n\nThe script applies the Installation custom resource and marks it with the `action=install` label, which triggers the Kyma installation.\n\n>**NOTE:** The Kyma installation runs in the background. Execute the `./installation/scripts/is-installed.sh` script to follow the installation process.\n","type":"Installation"},{"order":"034-inst-install-subcomponents","title":"Install subcomponents","source":"\nIt is up to you to decide which subcomponents you install as part of the `core` release. By default, most of the core subcomponents are enabled. If you want to install only specific subcomponents, follow the steps that you need to perform before the local and cluster installation.\n\n## Install subcomponents locally\n\nTo specify whether to install a given core subcomponent on Minikube, use the `manage-component.sh` script before you trigger the Kyma installation. The script consumes two parameters:\n\n- the name of the core subcomponent\n- a Boolean value that determines whether to install the subcomponent (`true`) or not (`false`)\n\nExample:\n\nTo enable the Azure Broker subcomponent, run the following command:\n```\nscripts/manage-component.sh azure-broker true\n```\n\nAlternatively, to disable the Azure Broker subcomponent, run this command:\n```\nscripts/manage-component.sh azure-broker false\n```\n\n## Install subcomponents on a cluster\n\nInstall subcomponents on a cluster based on Helm conditions described in the `requirements.yaml` file. Read more about the fields in the `requirements.yaml` file [here](https://github.com/helm/helm/blob/release-2.10/docs/charts.md#tags-and-condition-fields-in-requirementsyaml).\n\nTo specify whether to install a given core subcomponent, provide override values before you trigger the installation.\n\nExample:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kyma-sub-components\n  namespace: kyma-installer\n  labels:\n    installer: overrides\ndata:\n  azure-broker.enabled: \"true\"\n```\n\n>**NOTE:** Some subcomponents can require additional configuration to work properly.\n\n## Specify subcomponents versions\n\nVersions of the Kyma components are specified in the `values.yaml` file in charts. Two properties, `version` and `dir`, describe each component version. The first one defines the actual docker image tag. The second property describes the directory under which the tagged image is pushed. It is optional and is followed by a forward slash (/).\n\nPossible values of the `dir` property:\n- `pr/` contains images built from the pull request\n- `develop/` contains images built from the `master` branch\n- `rc/` contains images built for a pre-release\n- `` (empty) contains images built for a release\n\nTo override subcomponents versions during Kyma startup, create the `versions-overrides.env` file in the `installation` directory.\n\nThe example overrides the `Environments` component and sets the image version to `0.0.1`, based on the version from the `develop` directory.\n\nExample:\n\n```\nglobal.environments.dir=develop/\nglobal.environments.version=0.0.1\n```\n","type":"Installation"},{"order":"035-inst-local-reinstallation","title":"Reinstall Kyma","source":"\nThe custom scripts allow you to remove Kyma from a Minikube cluster and reinstall Kyma without removing the cluster.\n\n> **NOTE:** These scripts do not delete the cluster from your Minikube. This allows you to quickly reinstall Kyma.\n\n1. Use the `clean-up.sh` script to uninstall Kyma from the cluster. Run:\n  ```\n  scripts/clean-up.sh\n  ```\n\n2. Run this script to reinstall Kyma on an existing cluster:\n  ```\n  cmd/run.sh --skip-minikube-start\n  ```\n","type":"Installation"},{"order":"037-inst-custom-istio","title":"Installation with custom Istio deployment","source":"\nYou can use Kyma with a custom deployment of Istio that you installed in the target environment. To enable such implementation, remove Istio from the list of components that install with Kyma.\nThe version of your Istio deployment must match the version that Kyma currently supports.\n\nIn the installation process, the installer applies a custom patch to every Istio deployment. This is a mandatory step.  \n\n>**NOTE:** To learn more, read the **Istio patch** document in the **Service Mesh** documentation topic.\n\n## Prerequisites\n\n- A live Istio version compatible with the version currently supported by Kyma. To check the supported version, see the `resources/istio-kyma-patch/templates/job.yaml` file.\n  >**NOTE:** Follow [this](https://istio.io/docs/setup/kubernetes/quick-start/) quick start guide to learn how to install and configure Istio on a Kubernetes cluster.\n\n- Security enabled in your Istio deployment. To verify if security is enabled, check if the `policies.authentication.istio.io` custom resource exists in the cluster.\n- Kyma downloaded from the latest [release](https://github.com/kyma-project/kyma/releases).\n\n## Local installation\n\n1. Remove these lines from the `kyma-config-local.yaml` file:\n  ```\n  name: \"istio\"\n  namespace: \"istio-system\"\n  ```\n2. Follow the installation steps described in the **Install Kyma locally from the release** document.\n\n## Cluster installation\n\n1. Remove these lines from the `kyma-config-cluster.yaml` file:\n  ```\n  name: \"istio\"\n  namespace: \"istio-system\"\n  ```\n2. Follow the installation steps described in the **Install Kyma on a GKE cluster** document.\n\n## Verify the installation\n\n1. Check if all Pods are running in the `kyma-system` Namespace:\n  ```\n  kubectl get pods -n kyma-system\n  ```\n2. Sign in to the Kyma Console using the `admin@kyma.cx` as described in the **Install Kyma locally from the release** document.\n","type":"Installation"},{"order":"040-gs-sample-service-deployment-to-local","title":"Sample service deployment on local","source":"\nThis Getting Started guide is intended for the developers who want to quickly learn how to deploy a sample service and test it with Kyma installed locally on Mac.\n\nThis guide uses a standalone sample service written in the [Go](http://golang.org) language .\n\n## Prerequisites\n\nTo use the Kyma cluster and install the example, download these tools:\n\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.10.0\n- [curl](https://github.com/curl/curl)\n\n## Steps\n\n### Deploy and expose a sample standalone service\n\nFollow these steps:\n\n1. Deploy the sample service to any of your Environments. Use the `stage` Environment for this guide:\n\n   ```bash\n   kubectl create -n stage -f https://raw.githubusercontent.com/kyma-project/examples/master/http-db-service/deployment/deployment.yaml\n   ```\n\n2. Create an unsecured API for your example service:\n\n   ```bash\n   kubectl apply -n stage -f https://raw.githubusercontent.com/kyma-project/examples/master/gateway/service/api-without-auth.yaml\n   ```\n\n3. Add the IP address of Minikube to the `hosts` file on your local machine for your APIs:\n\n   ```bash\n   $ echo \"$(minikube ip) http-db-service.kyma.local\" | sudo tee -a /etc/hosts\n   ```\n\n4. Access the service using the following call:\n   ```bash\n   curl -ik https://http-db-service.kyma.local/orders\n   ```\n\n   The system returns a response similar to the following:\n   ```\n   HTTP/2 200\n   content-type: application/json;charset=UTF-8\n   vary: Origin\n   date: Mon, 01 Jun 2018 00:00:00 GMT\n   content-length: 2\n   x-envoy-upstream-service-time: 131\n   server: envoy\n\n   []\n   ```\n\n### Update your service's API to secure it\n\nRun the following command:\n\n   ```bash\n   kubectl apply -n stage -f https://raw.githubusercontent.com/kyma-project/examples/master/gateway/service/api-with-auth.yaml\n   ```\nAfter you apply this update, you must include a valid bearer ID token in the Authorization header to access the service.\n\n>**NOTE:** The update might take some time.\n","type":"Getting Started"},{"order":"041-gs-sample-service-deployment-to-cluster","title":"Sample service deployment on a cluster","source":"\nThis Getting Started guide is intended for the developers who want to quickly learn how to deploy a sample service and test it with the Kyma cluster.\n\nThis guide uses a standalone sample service written in the [Go](http://golang.org) language.\n\n## Prerequisites\n\nTo use the Kyma cluster and install the example, download these tools:\n\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.10.0\n- [curl](https://github.com/curl/curl)\n\n## Steps\n\n### Download configuration for kubectl\n\nFollow these steps to download **kubeconfig** and configure kubectl to access the Kyma cluster:\n1. Access the Console UI and download the **kubectl** file from the settings page.\n2. Place downloaded file in the following location: `$HOME/.kube/kubeconfig`.\n3. Point **kubectl** to the configuration file using the terminal: `export KUBECONFIG=$HOME/.kube/kubeconfig`.\n4. Confirm **kubectl** is configured to use your cluster: `kubectl cluster-info`.\n\n### Set the cluster domain variable\n\nThe commands throughout this guide use URLs that require you to provide the domain of the cluster which you are using. To complete this configuration, set the variable `yourClusterDomain` to the domain of your cluster.\n\nFor example, if your cluster's domain is `demo.cluster.kyma.cx`, run the following command:\n\n   ```bash\n   export yourClusterDomain='demo.cluster.kyma.cx'\n   ```\n\n### Deploy and expose a sample standalone service\n\nFollow these steps:\n\n1. Deploy the sample service to any of your Environments. Use the `stage` Environment for this guide:\n\n   ```bash\n   kubectl create -n stage -f https://minio.$yourClusterDomain/content/root/kyma/assets/deployment.yaml\n   ```\n\n2. Create an unsecured API for your service:\n\n   ```bash\n   curl -k https://minio.$yourClusterDomain/content/root/kyma/assets/api-without-auth.yaml |  sed \"s/.kyma.local/.$yourClusterDomain/\" | kubectl apply -n stage -f -\n   ```\n\n3. Access the service using the following call:\n   ```bash\n   curl -ik https://http-db-service.$yourClusterDomain/orders\n   ```\n\n   The system returns a response similar to the following:\n   ```\n   HTTP/2 200\n   content-type: application/json;charset=UTF-8\n   vary: Origin\n   date: Mon, 01 Jun 2018 00:00:00 GMT\n   content-length: 2\n   x-envoy-upstream-service-time: 131\n   server: envoy\n\n   []\n   ```\n\n### Update your service's API to secure it\n\nRun the following command:\n\n   ```bash\n   curl -k https://minio.$yourClusterDomain/content/root/kyma/assets/api-with-auth.yaml |  sed \"s/.kyma.local/.$yourClusterDomain/\" | kubectl apply -n stage -f -\n   ```\nAfter you apply this update, you must include a valid bearer ID token in the Authorization header to access the service.\n\n>**NOTE:** The update might take some time.\n","type":"Getting Started"},{"order":"042-gs-local-develop-no-docker","title":"Develop a service locally without using Docker","source":"\nYou can develop services in the local Kyma installation without extensive Docker knowledge or a need to build and publish a Docker image. The `minikube mount` feature allows you to mount a directory from your local disk into the local Kubernetes cluster.\n\nThis guide shows how to use this feature, using the service example implemented in Golang.\n\n## Prerequisites\n\nInstall [Golang](https://golang.org/dl/).\n\n## Steps\n\n### Install the example on your local machine\n\n1. Install the example:\n```shell\ngo get -insecure github.com/kyma-project/examples/http-db-service\n```\n2. Navigate to installed example and the `http-db-service` folder inside it:\n```shell\ncd ~/go/src/github.com/kyma-project/examples/http-db-service\n```\n3. Build the executable to run the application:\n```shell\nCGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n```\n\n### Mount the example directory into Minikube\n\nFor this step, you need a running local Kyma instance. Read the **Install Kyma locally from the release** Installation guide to learn how to install Kyma locally.\n\n1. Open the terminal window. Do not close it until the development finishes.\n2. Mount your local drive into Minikube:\n```shell\n# Use the following pattern:\nminikube mount {LOCAL_DIR_PATH}:{CLUSTER_DIR_PATH}`\n# To follow this guide, call:\nminikube mount ~/go/src/github.com/kyma-project/examples/http-db-service:/go/src/github.com/kyma-project/examples/http-db-service\n```\n\nSee the example and expected result:\n```shell\n# Terminal 1\n$ minikube mount ~/go/src/github.com/kyma-project/examples/http-db-service:/go/src/github.com/kyma-project/examples/http-db-service\n\nMounting /Users/{USERNAME}/go/src/github.com/kyma-project/examples/http-db-service into /go/src/github.com/kyma-project/examples/http-db-service on the minikube VM\nThis daemon process must stay alive for the mount to still be accessible...\nufs starting\n```\n\n### Run your local service inside Minikube\n\n1. Create Pod that uses the base Golang image to run your executable located on your local machine:\n```shell\n# Terminal 2\nkubectl run mydevpod --image=golang:1.9.2-alpine --restart=Never -n stage --overrides='\n{\n   \"spec\":{\n      \"containers\":[\n         {\n            \"name\":\"mydevpod\",\n            \"image\":\"golang:1.9.2-alpine\",\n            \"command\": [\"./main\"],\n            \"workingDir\":\"/go/src/github.com/kyma-project/examples/http-db-service\",\n            \"volumeMounts\":[\n               {\n                  \"mountPath\":\"/go/src/github.com/kyma-project/examples/http-db-service\",\n                  \"name\":\"local-disk-mount\"\n               }\n            ]\n         }\n      ],\n      \"volumes\":[\n         {\n            \"name\":\"local-disk-mount\",\n            \"hostPath\":{\n               \"path\":\"/go/src/github.com/kyma-project/examples/http-db-service\"\n            }\n         }\n      ]\n   }\n}\n'\n```\n2. Expose the Pod as a service from Minikube to verify it:\n```shell\nkubectl expose pod mydevpod --name=mypodservice --port=8017 --type=NodePort -n stage\n```\n3. Check the Minikube IP address and Port, and use them to access your service.\n```shell\n# Get the IP address.\nminikube ip\n# See the example result: 192.168.64.44\n# Check the Port.\nkubectl get services -n stage\n# See the example result: mypodservice  NodePort 10.104.164.115  <none>  8017:32226/TCP  5m\n```\n4. Call the service from your terminal.\n```shell\ncurl {minikube ip}:{port}/orders -v\n# See the example: curl http://192.168.64.44:32226/orders -v\n# The command returns an empty array.\n```\n\n### Modify the code locally and see the results immediately in Minikube\n\n1. Edit the `main.go` file by adding a new `test` endpoint to the `startService` function\n```go\nrouter.HandleFunc(\"/test\", func (w http.ResponseWriter, r *http.Request) {\n\tw.Write([]byte(\"test\"))\n})\n```\n2. Build a new executable to run the application inside Minikube:\n```shell\nCGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n```\n3. Replace the existing Pod with the new version:\n```shell\nkubectl get pod mydevpod -n stage -o yaml | kubectl replace --force -f -\n```\n4. Call the new `test` endpoint of the service from your terminal. The command returns the `Test` string:\n```shell\ncurl http://192.168.64.44:32226/test -v\n```\n","type":"Getting Started"},{"order":"043-gs-publish-service-image-and-deploy","title":"Publish a service Docker image and deploy it to Kyma","source":"\nIn the Getting Started guide for local development of a service, you can learn how to develop a service locally. You can immediately see all the changes made in the local Kyma installation based on Minikube, without building a Docker image and publishing it to a Docker registry, such as the Docker Hub.\n\nUsing the same example service, this guide explains how to build a Docker image for your service, publish it to the Docker registry, and deploy it to the local Kyma installation. The instructions base on Minikube, but you can also use the image that you create, and the Kubernetes resource definitions that you use on the Kyma cluster.\n\n>**NOTE:** The deployment works both on local Kyma installation and on the Kyma cluster.\n\n## Steps\n\n### Build a Docker image\n\nThe `http-db-service` example used in this guide provides you with the `Dockerfile` necessary for building Docker images. Examine the `Dockerfile` to learn how it looks and how it uses the Docker Multistaging feature, but do not use it one-to-one for production. There might be custom `LABEL` attributes with values to override.\n\n1. In your terminal, go to the `examples/http-db-service` directory. If you did not follow the **Sample service deployment on local** guide and you do not have this directory locally, get the `http-db-service` example from the `examples` repository.\n2. Run the build with `./build.sh`.\n\n>**NOTE:** Ensure that the new image builds and is available in your local Docker registry by calling `docker images`. Find an image called `example-http-db-service` and tagged as `latest`.\n\n### Register the image in the Docker Hub\n\nThis guide bases on Docker Hub. However, there are many other Docker registries available. You can use a private Docker registry, but it must be available in the Internet. For more details about using a private Docker registry, see the **How to deploy a Docker image from a private registry** document.\n\n1. Open the [Docker Hub](https://hub.docker.com/) webpage.\n2. Provide all of the required details and sign up.\n\n### Sign in to the Docker Hub registry in the terminal\n\n1. Call `docker login`.\n2. Provide the username and password, and select the `ENTER` key.\n\n### Push the image to the Docker Hub\n\n1. Tag the local image with a proper name required in the registry: `docker tag example-http-db-service {username}/example-http-db-service:0.0.1`.\n2. Push the image to the registry: `docker push {username}/example-http-db-service:0.0.1`.\n```shell\n#This is how it looks in the terminal\n\nThe push refers to repository [docker.io/{username}/example-http-db-service]\n4302273b9e11: Pushed\n5835bd463c0e: Pushed\n0.0.1: digest: sha256:9ec28342806f50b92c9b42fa36d979c0454aafcdda6845b362e2efb9816d1439 size: 734\n```\n>**NOTE:** To verify if the image is successfully published, check if it is available online at the following address: `https://hub.docker.com/r/{username}/example-http-db-service/`\n\n### Deploy to Kyma\n\nThe `http-db-service` example contains sample Kubernetes resource definitions needed for the basic Kyma deployment. Find them in the `deployment` folder. Perform the following modifications to use your newly-published image in the local Kyma installation:\n\n1. Go to the `deployment` directory.\n2. Edit the `deployment.yaml` file. Change the **image** attribute to `{username}/example-http-db-service:0.0.1`.\n3. Create the new resources in local Kyma using these commands: `kubectl create -f deployment.yaml -n stage && kubectl create -f ingress.yaml -n stage`.\n4. Edit your `/etc/hosts` to add the new `http-db-service.kyma.local` host to the list of hosts associated with your `minikube ip`. Follow these steps:\n    - Open a terminal window and run: `sudo vim /etc/hosts`\n    - Select the **i** key to insert a new line at the top of the file.\n    - Add this line: `{YOUR.MINIKUBE.IP} http-db-service.kyma.local`\n    - Type `:wq` and select the **Enter** key to save the changes.\n5. Run this command to check if you can access the service: `curl https://http-db-service.kyma.local/orders`. The response should return an empty array.\n","type":"Getting Started"},{"order":"044-gs-installation-overrides","title":"Helm overrides for Kyma installation","source":"\nKyma packages its components into [Helm](https://github.com/helm/helm/tree/master/docs) charts that the [Installer](../../../components/installer/README.md) uses.\nThis document describes how to configure the Installer with override values for Helm [charts](https://github.com/helm/helm/blob/master/docs/charts.md).\n\n\n## Overview\n\nThe Installer is a Kubernetes Operator that uses Helm to install Kyma components.\nHelm provides an overrides feature to customize the installation of charts, such as to configure environment-specific values.\nWhen using Installer for Kyma installation, users can't interact with Helm directly. The installation is not an interactive process.\n\nTo customize the Kyma installation, the Installer exposes a generic mechanism to configure Helm overrides called **user-defined** overrides.\n\n\n## User-defined overrides\n\nThe Installer finds user-defined overrides by reading the ConfigMaps and Secrets deployed in the `kyma-installer` Namespace and marked with the `installer:overrides` Label.\n\nThe Installer constructs a single override by inspecting the ConfigMap or Secret entry key name. The key name should be a dot-separated sequence of strings corresponding to the structure of keys in the chart's `values.yaml` file or the entry in chart's template. See the examples below.\n\nInstaller merges all overrides recursively into a single YAML stream and passes it to Helm during the Kyma installation/upgrade operation.\n\n\n## Common vs component overides\n\nThe Installer looks for available overrides each time a component installation or update operation is due.\nOverrides for the component are composed from two sets: **common** overrides and **component-specific** overrides.\n\nKyma uses common overrides for the installation of all components. ConfigMaps and Secrets marked with the label `installer:overrides`, contain the definition. They require no additional label.\n\nKyma uses component-specific overrides only for the installation of specific components. ConfigMaps and Secrets marked with both `installer:overrides` and `component: <name>` Labels, where `<name>` is the component name, contain the definition. Component-specific overrides have precedence over Common ones in case of conflicting entries.\n\n\n## Overrides Examples\n\n### Top-level charts overrides\n\nOverrides for top-level charts are straightforward. Just use the template value from the chart (without leading \".Values.\" prefix) as the entry key in the ConfigMap or Secret.\n\nExample:\n\nThe Installer uses a `core` top-level chart that contains a template with the following value reference:\n```\nmemory: {{ .Values.test.acceptance.ui.requests.memory }}\n```\nThe chart's default value `test.acceptance.ui.requests.memory` in the `values.yaml` file resolves the template.\nThe following fragment of `values.yaml` shows this definition:\n```\ntest:\n  acceptance:\n    ui:\n      requests:\n        memory: \"1Gi\"\n```\n\nTo override this value, for example to \"2Gi\", proceed as follows:\n- Create a ConfigMap in the `kyma-installer` Namespace, labelled with: `installer:overrides` (or reuse an existing one).\n- Add an entry `test.acceptance.ui.requests.memory: 2Gi` to the map.\n\nOnce the installation starts, the Installer generates overrides based on the map entries. The system uses the value of \"2Gi\" instead of the default \"1Gi\" from the chart `values.yaml` file.\n\nFor overrides that the system should keep in Secrets, just define a Secret object instead of a ConfigMap with the same key and a base64-encoded value. Be sure to label the Secret with `installer:overrides`.\n\n\n### Sub-chart overrides\n\nOverrides for sub-charts follow the same convention as top-level charts. However, overrides require additional information about sub-chart location.\n\nWhen a sub-chart contains the `values.yaml` file, the information about the chart location is not necessary because the chart and it's `values.yaml` file are on the same level in the directory hierarchy.\n\nThe situation is different when the Installer installs a chart with sub-charts.\nAll template values for a sub-chart must be prefixed with a sub-chart \"path\" that is relative to the top-level \"parent\" chart.\n\nThis is not an Installer-specific requirement. The same considerations apply when you provide overrides manually using the `helm` command-line tool.\n\nHere is an example.\nThere's a `core` top-level chart, that the Installer installs.\nThere's an `application-connector` sub-chart in `core` with another nested sub-chart: `connector-service`.\nIn one of its templates there's a following fragment (shortened):\n\n```\nspec:\n  containers:\n  - name: {{ .Chart.Name }}\n\targs:\n\t  - \"/connectorservice\"\n\t  - '--appName={{ .Chart.Name }}'\n\t  - \"--domainName={{ .Values.global.domainName }}\"\n\t  - \"--tokenExpirationMinutes={{ .Values.deployment.args.tokenExpirationMinutes }}\"\n```\n\nThe following fragment of the `values.yaml` file in `connector-service` chart defines the default value for `tokenExpirationMinutes`:\n\n```\ndeployment:\n  args:\n    tokenExpirationMinutes: 60\n```\n\nTo override this value, such as to change \"60 to \"90\", do the following:\n\n- Create a ConfigMap in the `kyma-installer` Namespace labeled with `installer:overrides` or reuse existing one.\n- Add an entry `application-connector.connector-service.deployment.args.tokenExpirationMinutes: 90` to the map.\n\nNotice that the user-provided override key now contains two parts:\n\n- The chart \"path\" inside the top-level `core` chart: `application-connector.connector-service`\n- The original template value reference from the chart without the .Values. prefix: `deployment.args.tokenExpirationMinutes`.\n\nOnce the installation starts, the Installer generates overrides based on the map entries. The system uses the value of \"90\" instead of the default value of \"60\" from the `values.yaml` chart file.\n\n\n## Global overrides\n\nThere are several important parameters usually shared across the charts.\nHelm convention to provide these requires the use of the `global` override key.\nFor example, to define the `global.domain` override, just use \"global.domain\" as the name of the key in ConfigMap or Secret for the Installer.\n\nOnce the installation starts, the Installer merges all of the map entries and collects all of the global entries under the `global` top-level key to use for installation.\n\n\n## Values and types\n\nInstaller generally recognizes all override values as strings. It internally renders overrides to Helm as a YAML stream with only string values.\n\nThere is one exception to this rule with respect to handling booleans:\nThe system converts \"true\" or \"false\" strings that it encounters to a corresponding boolean value (true/false).\n\n\n## Merging and conflicting entries\n\nWhen the Installer encounters two overrides with the same key prefix, it tries to merge them.\nIf both of them represent a map (they have nested sub-keys), their nested keys are recursively merged.\nIf at least one of keys points to a final value, the Installer performs the merge in a non-deterministic order, so either one of the overrides is rendered in the final YAML data.\n\nIt is important to avoid overrides having the same keys for final values.\n\n\n### Example of non-conflicting merge:\n\nTwo overrides with a common key prefix (\"a.b\"):\n\n```\n\"a.b.c\": \"first\"\n\"a.b.d\": \"second\"\n```\n\nThe Installer yields correct output:\n\n```\na:\n  b:\n    c: first\n    d: second\n```\n\n### Example of conflicting merge:\n\nTwo overrides with the same key (\"a.b\"):\n\n```\n\"a.b\": \"first\"\n\"a.b\": \"second\"\n```\n\nThe Installer yields either:\n\n```\na:\n  b: \"first\"\n```\n\nOr (due to non-deterministic merge order):\n\n```\na:\n  b: \"second\"\n```\n","type":"Getting Started"},{"order":"050-cr-installation","title":"Installation","source":"\nThe `installations.installer.kyma-project.io` Custom Resource Definition (CRD) is a detailed description of the kind of data and the format used to control the Kyma Installer, a proprietary solution based on the\n[Kubernetes operator](https://coreos.com/operators/) principles. To get the up-to-date CRD and show the output in the `yaml` format, run this command:  \n\n```\nkubectl get crd installations.installer.kyma-project.io -o yaml\n```\n\n## Sample Custom Resource\n\nThis is a sample CR that controls the Kyma installer. This example has the **action** label set to `install`, which means that it triggers the installation of Kyma. The  **name** and **namespace**  fields in the `components` array define which components you install and Namespaces in which you install them. This example shows that you install the `hmc-default` release of the `remote-environments` component in the `kyma-integration` Namespace. \n\n>**NOTE:** See the `installer-cr.yaml.tpl` file in the `/installation/resources` directory for the complete list of Kyma components.\n\n```\napiVersion: \"installer.kyma-project.io/v1alpha1\"\nkind: Installation\nmetadata:\n  name: kyma-installation\n  labels:\n    action: install\n  finalizers:\n    - finalizer.installer.kyma-project.io\nspec:\n  version: \"1.0.0\"\n  url: \"https://sample.url.com/kyma_release.tar.gz\"\n  components: \n    - name: \"cluster-essentials\"\n      namespace: \"kyma-system\"\n    - name: \"istio\"\n      namespace: \"istio-system\"\n    - name: \"prometheus-operator\"\n      namespace: \"kyma-system\"\n    - name: \"provision-bundles\"\n    - name: \"dex\"\n      namespace: \"kyma-system\"\n    - name: \"core\"\n      namespace: \"kyma-system\"\n    - name: \"remote-environments\"\n      namespace: \"kyma-integration\"\n      release: \"ec-default\"\n    - name: \"remote-environments\"\n      namespace: \"kyma-integration\"\n      release: \"hmc-default\"\n```\n\n## Custom resource parameters\n\nThis table lists all the possible parameters of a given resource together with their descriptions:\n\n| Field   |      Mandatory      |  Description |\n|:----------:|:-------------:|:------|\n| **metadata.name** | **YES** | Specifies the name of the CR. |\n| **metadata.labels.action** | **YES** | Defines the behavior of the Kyma installer. Available options are `install` and `uninstall`. |\n| **metadata.finalizers** | **NO** | Protects the CR from deletion. Read [this](https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#finalizers) Kubernetes document to learn more about finalizers. |\n| **spec.version** | **NO** | When manually installing Kyma on a cluster, specify any valid [SemVer](https://semver.org/) notation string. |\n| **spec.url** | **YES** | Specifies the location of the Kyma sources `tar.gz` package. For example, for the `master` branch of Kyma, the address is `https://github.com/kyma-project/kyma/archive/master.tar.gz` |\n| **spec.components** | **YES** | Lists which components of Helm chart components to install or update. |\n| **spec.components.name** | **YES** | Specifies the name of the component which is the same as the name of the component subdirectory in the `resources` directory. |\n| **spec.components.namespace** | **YES** | Defines the Namespace in which you want the Installer to install, or update the component. |\n| **spec.components.release** | **NO** | Provides the name of the Helm release. The default parameter is the component name. |\n","type":"Custom Resource"}]},"navigation":{"topics":[{"id":"kyma","contentType":"root","sections":[{"topicType":"Overview","name":"Overview","anchor":"overview","titles":[{"name":"In a nutshell","anchor":"in-a-nutshell"},{"name":"Main features","anchor":"main-features"},{"name":"Technology stack","anchor":"technology-stack"},{"name":"Key components","anchor":"key-components"},{"name":"Kyma and Knative - brothers in arms","anchor":"kyma-and-knative---brothers-in-arms"},{"name":"How to start","anchor":"how-to-start"}]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Components","anchor":"components"},{"name":"Environments","anchor":"environments"},{"name":"Testing Kyma","anchor":"testing-kyma"},{"name":"Charts","anchor":"charts"},{"name":"Deploy with a private Docker registry","anchor":"deploy-with-a-private-docker-registry"}]},{"topicType":"Installation","name":"Installation","anchor":"installation","titles":[{"name":"Install Kyma locally from the release","anchor":"install-kyma-locally-from-the-release"},{"name":"Install Kyma locally from sources","anchor":"install-kyma-locally-from-sources"},{"name":"Install Kyma on a GKE cluster","anchor":"install-kyma-on-a-gke-cluster"},{"name":"Local installation scripts","anchor":"local-installation-scripts"},{"name":"Install subcomponents","anchor":"install-subcomponents"},{"name":"Reinstall Kyma","anchor":"reinstall-kyma"},{"name":"Installation with custom Istio deployment","anchor":"installation-with-custom-istio-deployment"}]},{"topicType":"Getting Started","name":"Getting Started","anchor":"getting-started","titles":[{"name":"Sample service deployment on local","anchor":"sample-service-deployment-on-local"},{"name":"Sample service deployment on a cluster","anchor":"sample-service-deployment-on-a-cluster"},{"name":"Develop a service locally without using Docker","anchor":"develop-a-service-locally-without-using-docker"},{"name":"Publish a service Docker image and deploy it to Kyma","anchor":"publish-a-service-docker-image-and-deploy-it-to-kyma"},{"name":"Helm overrides for Kyma installation","anchor":"helm-overrides-for-kyma-installation"}]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"Installation","anchor":"installation"}]}]},{"id":"security","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Kubeconfig generator","anchor":"kubeconfig-generator"},{"name":"Add an Identity Provider to Dex","anchor":"add-an-identity-provider-to-dex"},{"name":"Manage static users in Dex","anchor":"manage-static-users-in-dex"},{"name":"Update TLS certificate","anchor":"update-tls-certificate"}]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"Group","anchor":"group"},{"name":"Identity Provider Presets","anchor":"identity-provider-presets"}]}]},{"id":"service-catalog","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Resources","anchor":"resources"},{"name":"Add a service to the Catalog","anchor":"add-a-service-to-the-catalog"},{"name":"Provisioning and binding","anchor":"provisioning-and-binding"},{"name":"Etcd Database","anchor":"etcd-database"}]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]},{"topicType":null,"name":"CLI reference","anchor":"cli-reference","titles":[]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"ServiceBindingUsage","anchor":"servicebindingusage"},{"name":"UsageKind","anchor":"usagekind"}]},{"topicType":"Getting Started","name":"Getting Started","anchor":"getting-started","titles":[{"name":"Register a ClusterServiceBroker","anchor":"register-a-clusterservicebroker"}]}]},{"id":"service-brokers","contentType":"components","sections":[{"topicType":"Overview","name":"Overview","anchor":"overview","titles":[{"name":"Overview","anchor":"overview"},{"name":"Azure Broker","anchor":"azure-broker"},{"name":"Remote Environment Broker","anchor":"remote-environment-broker"},{"name":"Helm Broker","anchor":"helm-broker"}]},{"topicType":"Configuration","name":"Configuration","anchor":"configuration","titles":[{"name":"Configure Helm Broker","anchor":"configure-helm-broker"},{"name":"How to create a bundle","anchor":"how-to-create-a-bundle"},{"name":"Binding bundles","anchor":"binding-bundles"},{"name":"Enable the Azure Broker for local deployment","anchor":"enable-the-azure-broker-for-local-deployment"}]},{"topicType":"Architecture","name":"Architecture","anchor":"architecture","titles":[{"name":"The Remote Environment Broker architecture","anchor":"the-remote-environment-broker-architecture"},{"name":"Helm Broker architecture","anchor":"helm-broker-architecture"}]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"EventActivation","anchor":"eventactivation"}]}]},{"id":"application-connector","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":"Architecture","name":"Architecture","anchor":"architecture","titles":[{"name":"Application Connector components","anchor":"application-connector-components"},{"name":"Connector Service","anchor":"connector-service"},{"name":"Proxy Service","anchor":"proxy-service"}]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Security","anchor":"security"},{"name":"Access the Application Connector on a local Kyma deployment","anchor":"access-the-application-connector-on-a-local-kyma-deployment"},{"name":"Consume applications through the Service Catalog","anchor":"consume-applications-through-the-service-catalog"},{"name":"Metadata Service","anchor":"metadata-service"},{"name":"Pass an access token in a request header","anchor":"pass-an-access-token-in-a-request-header"}]},{"topicType":"API","name":"API","anchor":"api","titles":[{"name":"Connector Service","anchor":"connector-service"},{"name":"Metadata Service","anchor":"metadata-service"},{"name":"Event Service","anchor":"event-service"}]},{"topicType":"Getting Started","name":"Getting Started","anchor":"getting-started","titles":[{"name":"Create a new Remote Environment","anchor":"create-a-new-remote-environment"},{"name":"Get the client certificate","anchor":"get-the-client-certificate"},{"name":"Register a service","anchor":"register-a-service"},{"name":"Bind a Remote Environment to an Environment","anchor":"bind-a-remote-environment-to-an-environment"},{"name":"Trigger a lambda with events","anchor":"trigger-a-lambda-with-events"},{"name":"Call a registered external service from Kyma","anchor":"call-a-registered-external-service-from-kyma"}]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"RemoteEnvironment","anchor":"remoteenvironment"},{"name":"EnvironmentMapping","anchor":"environmentmapping"}]}]},{"id":"event-bus","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Basic concepts","anchor":"basic-concepts"},{"name":"Event flow requirements","anchor":"event-flow-requirements"},{"name":"Service Programming Model","anchor":"service-programming-model"},{"name":"Troubleshooting","anchor":"troubleshooting"},{"name":"Subscription updates","anchor":"subscription-updates"}]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"Subscription","anchor":"subscription"}]},{"topicType":null,"name":"CLI reference","anchor":"cli-reference","titles":[]}]},{"id":"service-mesh","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Sidecar Proxy Injection","anchor":"sidecar-proxy-injection"},{"name":"Istio patch","anchor":"istio-patch"}]}]},{"id":"serverless","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Custom Resources","anchor":"custom-resources"},{"name":"Managing Lambdas","anchor":"managing-lambdas"},{"name":"The Node.js Programming Model","anchor":"the-node.js-programming-model"}]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]},{"topicType":null,"name":"CLI reference","anchor":"cli-reference","titles":[]}]},{"id":"monitoring","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]},{"topicType":"Getting Started","name":"Getting Started","anchor":"getting-started","titles":[{"name":"Expose custom metrics in Kyma","anchor":"expose-custom-metrics-in-kyma"}]}]},{"id":"tracing","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Propagate HTTP headers","anchor":"propagate-http-headers"},{"name":"Trace Comparison","anchor":"trace-comparison"}]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]}]},{"id":"api-gateway","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Security","anchor":"security"}]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"Api","anchor":"api"}]}]},{"id":"logging","contentType":"components","sections":[{"topicType":null,"name":"Overview","anchor":"overview","titles":[]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"Access OK Log","anchor":"access-ok-log"}]},{"topicType":null,"name":"Architecture","anchor":"architecture","titles":[]}]},{"id":"console","contentType":"components","sections":[{"topicType":"Overview","name":"Overview","anchor":"overview","titles":[{"name":"Overview","anchor":"overview"}]},{"topicType":"Details","name":"Details","anchor":"details","titles":[{"name":"UI extensibility","anchor":"ui-extensibility"}]},{"topicType":"Custom Resource","name":"Custom Resource","anchor":"custom-resource","titles":[{"name":"MicroFrontend","anchor":"microfrontend"},{"name":"ClusterMicroFrontend","anchor":"clustermicrofrontend"}]}]}]},"manifest":{"root":[{"displayName":"Kyma","id":"kyma"}],"components":[{"displayName":"Security","id":"security"},{"displayName":"Service Catalog","id":"service-catalog"},{"displayName":"Service Brokers","id":"service-brokers"},{"displayName":"Application Connector","id":"application-connector"},{"displayName":"Event Bus","id":"event-bus"},{"displayName":"Service Mesh","id":"service-mesh"},{"displayName":"Serverless","id":"serverless"},{"displayName":"Monitoring","id":"monitoring"},{"displayName":"Tracing","id":"tracing"},{"displayName":"API Gateway","id":"api-gateway"},{"displayName":"Logging","id":"logging"},{"displayName":"Console","id":"console"}]},"assetsPath":"/assets/docs/0.5/kyma/docs/assets/","locale":"en"}}